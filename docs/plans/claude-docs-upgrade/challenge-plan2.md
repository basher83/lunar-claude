# Claude Docs Script Variations Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Create three standalone Python scripts demonstrating different web scraping approaches (Jina Reader API, Jina MCP, Firecrawl MCP) as variations of `claude_docs.py`, plus supporting documentation.

**Architecture:**
- Each script is self-contained with uv script headers for portability
- Jina Reader: Direct HTTP calls using `requests` library (sequential)
- Jina MCP: Claude Agent SDK orchestrating parallel URL reads via Jina MCP tools
- Firecrawl MCP: Claude Agent SDK using Firecrawl tools for robust scraping

**Tech Stack:**
- Python 3.11+
- uv (script runner)
- requests (HTTP client for Jina Reader)
- claude-agent-sdk (for MCP-based scripts)
- typer (CLI framework)
- rich (terminal UI)
- pytest (testing)

**Success Criteria:**
- 3 standalone scripts that successfully download Claude Code docs
- Each script demonstrates its specific method's strengths
- Supporting documentation explains when to use each script
- All scripts follow best practices from research and SDK patterns

---

## Task 1: Create jina_reader_docs.py (Direct HTTP Calls)

**Files:**
- Create: `plugins/meta/claude-docs/scripts/jina_reader_docs.py`
- Test: `plugins/meta/claude-docs/tests/test_jina_reader_docs.py`
- Reference: `plugins/meta/claude-docs/scripts/claude_docs.py` (existing)

### Step 1: Write the failing test

Create test file structure to verify script behavior:

```python
#!/usr/bin/env python3
"""Tests for jina_reader_docs.py script."""

import pytest
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock
import sys

# Add scripts directory to path for imports
SCRIPTS_DIR = Path(__file__).parent.parent / "scripts"
sys.path.insert(0, str(SCRIPTS_DIR))

def test_jina_reader_url_construction():
    """Test that Jina Reader URLs are constructed correctly."""
    from jina_reader_docs import build_jina_reader_url

    url = "https://docs.claude.com/en/docs/claude-code/skills.md"
    reader_url = build_jina_reader_url(url)

    assert reader_url == "https://r.jina.ai/https://docs.claude.com/en/docs/claude-code/skills.md"

def test_api_key_auto_detection():
    """Test that API key is auto-detected from environment."""
    from jina_reader_docs import get_api_key

    with patch.dict('os.environ', {'JINA_API_KEY': 'test-key-123'}):
        api_key = get_api_key(cli_key=None)
        assert api_key == 'test-key-123'

def test_api_key_cli_override():
    """Test that CLI API key overrides environment."""
    from jina_reader_docs import get_api_key

    with patch.dict('os.environ', {'JINA_API_KEY': 'env-key'}):
        api_key = get_api_key(cli_key='cli-key')
        assert api_key == 'cli-key'

@patch('requests.get')
def test_download_with_jina_reader(mock_get):
    """Test downloading a page via Jina Reader API."""
    from jina_reader_docs import download_page_jina

    # Mock successful response
    mock_response = Mock()
    mock_response.status_code = 200
    mock_response.text = "# Test Content\n\nThis is test markdown."
    mock_response.headers = {"etag": "test-etag"}
    mock_get.return_value = mock_response

    url = "https://docs.claude.com/en/docs/claude-code/skills.md"
    success, content, metadata = download_page_jina(url, api_key="test-key")

    assert success is True
    assert "Test Content" in content
    assert metadata["etag"] == "test-etag"

@patch('requests.get')
def test_retry_logic_on_server_error(mock_get):
    """Test retry logic triggers on 5xx errors."""
    from jina_reader_docs import download_page_jina

    # Mock server error followed by success
    error_response = Mock()
    error_response.status_code = 503
    error_response.raise_for_status.side_effect = Exception("Server error")

    success_response = Mock()
    success_response.status_code = 200
    success_response.text = "# Success"
    success_response.headers = {"etag": "success-etag"}

    mock_get.side_effect = [error_response, success_response]

    url = "https://docs.claude.com/en/docs/claude-code/skills.md"
    success, content, metadata = download_page_jina(url, api_key="test-key", retries=2)

    assert success is True
    assert "Success" in content
```

### Step 2: Run test to verify it fails

Run: `pytest plugins/meta/claude-docs/tests/test_jina_reader_docs.py -v`

Expected: FAIL with "ModuleNotFoundError: No module named 'jina_reader_docs'"

### Step 3: Write minimal implementation

Create the script with core functionality:

```python
#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = [
#   "requests>=2.31.0",
#   "rich>=13.0.0",
#   "typer>=0.12.0",
# ]
# ///
"""
Download Claude Code documentation using Jina Reader API (direct HTTP calls).

This script demonstrates the direct HTTP approach using the Jina Reader API.
Best for: Simple scripts, automation without MCP setup, direct control over HTTP.

Usage:
    ./jina_reader_docs.py                              # Use free tier
    ./jina_reader_docs.py --api-key YOUR_KEY           # Use API key
    ./jina_reader_docs.py --output-dir docs --retries 5
"""

import json
import os
import sys
import time
from dataclasses import dataclass
from enum import Enum
from pathlib import Path

import requests
import typer
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.table import Table

class OutputFormat(str, Enum):
    """Output format options."""
    RICH = "rich"
    JSON = "json"

console = Console()

@dataclass
class FileMetadata:
    """Metadata for tracking file versions."""
    etag: str | None = None
    last_modified: str | None = None
    size: int = 0
    downloaded_at: float = 0.0

@dataclass
class DownloadResult:
    """Result of documentation download operation."""
    status: str
    downloaded: int = 0
    skipped: int = 0
    failed: int = 0
    duration_seconds: float = 0.0
    timestamp: str = ""

# Same constants as claude_docs.py
BASE_URL = "https://docs.claude.com/en/docs"
CLAUDE_CODE_BASE = f"{BASE_URL}/claude-code"
AGENTS_TOOLS_BASE = f"{BASE_URL}/agents-and-tools"

CLAUDE_CODE_PAGES = [
    "sub-agents", "plugins", "skills", "output-styles", "hooks-guide",
    "plugin-marketplaces", "settings", "statusline", "slash-commands",
    "hooks", "plugins-reference", "memory", "mcp"
]

AGENT_SKILLS_PAGES = [
    "agent-skills/overview",
    "agent-skills/quickstart",
    "agent-skills/best-practices",
]

DEFAULT_PAGES = [
    ("claude-code", page) for page in CLAUDE_CODE_PAGES
] + [
    ("agents-and-tools", page) for page in AGENT_SKILLS_PAGES
]

def build_jina_reader_url(url: str) -> str:
    """Construct Jina Reader API URL by prefixing with r.jina.ai."""
    return f"https://r.jina.ai/{url}"

def get_api_key(cli_key: str | None) -> str | None:
    """Get API key from CLI arg or environment, with CLI taking precedence."""
    if cli_key:
        return cli_key
    return os.environ.get("JINA_API_KEY")

def get_base_url(section: str) -> str:
    """Get the base URL for a documentation section."""
    if section == "claude-code":
        return CLAUDE_CODE_BASE
    elif section == "agents-and-tools":
        return AGENTS_TOOLS_BASE
    else:
        return f"{BASE_URL}/{section}"

def download_page_jina(
    url: str,
    api_key: str | None = None,
    retries: int = 3,
    timeout: int = 30,
) -> tuple[bool, str, dict]:
    """
    Download a page using Jina Reader API with retry logic.

    Args:
        url: Full URL to the documentation page
        api_key: Jina API key (optional, uses free tier if None)
        retries: Maximum retry attempts
        timeout: Request timeout in seconds

    Returns:
        Tuple of (success: bool, content: str, metadata: dict)
    """
    reader_url = build_jina_reader_url(url)
    headers = {"X-Return-Format": "markdown"}

    if api_key:
        headers["Authorization"] = f"Bearer {api_key}"

    for attempt in range(1, retries + 1):
        try:
            response = requests.get(reader_url, headers=headers, timeout=timeout)
            response.raise_for_status()

            metadata = {
                "etag": response.headers.get("etag"),
                "last_modified": response.headers.get("last-modified"),
                "size": len(response.text.encode('utf-8')),
                "downloaded_at": time.time(),
            }

            return True, response.text, metadata

        except requests.exceptions.HTTPError as e:
            if attempt < retries and e.response.status_code >= 500:
                wait_time = 2 ** (attempt - 1)
                console.print(
                    f"[yellow]âš [/yellow] HTTP {e.response.status_code}, "
                    f"retrying in {wait_time}s (attempt {attempt}/{retries})"
                )
                time.sleep(wait_time)
                continue
            else:
                console.print(
                    f"[red]âœ—[/red] HTTP {e.response.status_code}",
                    file=sys.stderr
                )
                return False, "", {}

        except requests.exceptions.RequestException as e:
            if attempt < retries:
                wait_time = 2 ** (attempt - 1)
                console.print(
                    f"[yellow]âš [/yellow] Network error, "
                    f"retrying in {wait_time}s (attempt {attempt}/{retries})"
                )
                time.sleep(wait_time)
                continue
            else:
                console.print(f"[red]âœ—[/red] {e}", file=sys.stderr)
                return False, "", {}

    return False, "", {}

def main(
    output_dir: Path = typer.Option(
        Path("./ai_docs"),
        "--output-dir", "-o",
        help="Directory to save downloaded files",
    ),
    api_key: str | None = typer.Option(
        None,
        "--api-key",
        help="Jina API key (defaults to JINA_API_KEY env var)",
    ),
    retries: int = typer.Option(
        3,
        "--retries", "-r",
        help="Maximum retry attempts per page",
        min=1, max=10,
    ),
    format: OutputFormat = typer.Option(
        OutputFormat.RICH,
        "--format",
        help="Output format: 'rich' or 'json'",
    ),
) -> None:
    """
    Download Claude Code docs using Jina Reader API (direct HTTP calls).

    This demonstrates the simplest approach: direct HTTP requests to Jina Reader.
    Best for scripts and automation without MCP setup.
    """
    output_dir.mkdir(exist_ok=True, parents=True)

    # Get API key (CLI overrides environment)
    api_key = get_api_key(api_key)

    if format == OutputFormat.RICH:
        key_status = "with API key" if api_key else "free tier (20 RPM)"
        console.print(
            f"[cyan]Downloading using Jina Reader API ({key_status})[/cyan]"
        )
        console.print(f"[dim]Output: {output_dir.absolute()}/[/dim]\n")

    start_time = time.time()
    success_count = 0
    failed_count = 0
    total_bytes = 0
    download_times = []

    pages = DEFAULT_PAGES

    if format == OutputFormat.RICH:
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console,
        ) as progress:
            task = progress.add_task("[cyan]Downloading...", total=len(pages))

            for section, page in pages:
                progress.update(task, description=f"[cyan]Downloading {page}...")

                base_url = get_base_url(section)
                url = f"{base_url}/{page}.md"

                page_start = time.time()
                success, content, metadata = download_page_jina(
                    url, api_key=api_key, retries=retries
                )
                duration = time.time() - page_start

                if success:
                    # Save to file
                    flat_filename = page.replace("/", "-")
                    output_file = output_dir / f"{flat_filename}.md"
                    output_file.write_text(content)

                    size_kb = metadata["size"] / 1024
                    speed_kbps = (metadata["size"] / 1024) / duration if duration > 0 else 0
                    console.print(
                        f"[green]âœ“[/green] {flat_filename}.md "
                        f"[dim]({size_kb:.1f}KB in {duration:.2f}s @ {speed_kbps:.1f}KB/s)[/dim]"
                    )
                    success_count += 1
                    total_bytes += metadata["size"]
                    download_times.append(duration)
                else:
                    failed_count += 1

                progress.advance(task)
    else:
        # JSON mode
        for section, page in pages:
            base_url = get_base_url(section)
            url = f"{base_url}/{page}.md"

            page_start = time.time()
            success, content, metadata = download_page_jina(
                url, api_key=api_key, retries=retries
            )
            duration = time.time() - page_start

            if success:
                flat_filename = page.replace("/", "-")
                output_file = output_dir / f"{flat_filename}.md"
                output_file.write_text(content)

                success_count += 1
                total_bytes += metadata["size"]
                download_times.append(duration)
            else:
                failed_count += 1

    total_time = time.time() - start_time

    # Output results
    if format == OutputFormat.JSON:
        result = {
            "status": "success" if failed_count == 0 else "error",
            "downloaded": success_count,
            "failed": failed_count,
            "duration_seconds": round(total_time, 2),
            "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
        }
        print(json.dumps(result))
    else:
        console.print()
        table = Table(title="Download Summary", show_header=True, header_style="bold cyan")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="green")

        table.add_row("Total Pages", str(len(pages)))
        table.add_row("âœ“ Downloaded", f"[green]{success_count}[/green]")
        if failed_count > 0:
            table.add_row("âœ— Failed", f"[red]{failed_count}[/red]")
        table.add_row("Total Size", f"{total_bytes / 1024:.1f} KB")
        table.add_row("Total Time", f"{total_time:.2f}s")

        if download_times:
            avg_time = sum(download_times) / len(download_times)
            table.add_row("Avg Time/Page", f"{avg_time:.2f}s")

        console.print(table)

        # Note about parallel potential
        if len(download_times) > 1:
            console.print(
                f"\n[yellow]ðŸ’¡ Note:[/yellow] Sequential processing. "
                f"See jina_mcp_docs.py for parallel downloads (~3x faster)."
            )

    if failed_count > 0:
        raise typer.Exit(code=1)

if __name__ == "__main__":
    typer.run(main)
```

### Step 4: Run test to verify it passes

Run: `pytest plugins/meta/claude-docs/tests/test_jina_reader_docs.py -v`

Expected: PASS (all tests green)

### Step 5: Manual integration test

Run: `./plugins/meta/claude-docs/scripts/jina_reader_docs.py --output-dir /tmp/test-jina-reader`

Expected: Successfully downloads 17 pages to /tmp/test-jina-reader/

### Step 6: Commit

```bash
git add plugins/meta/claude-docs/scripts/jina_reader_docs.py plugins/meta/claude-docs/tests/test_jina_reader_docs.py
git commit -m "feat: add jina_reader_docs.py script for direct HTTP scraping

- Implements direct Jina Reader API calls using requests
- Auto-detects JINA_API_KEY from environment
- Supports both free tier and API key usage
- Includes retry logic with exponential backoff
- Sequential processing with performance notes"
```

---

## Task 2: Create jina_mcp_docs.py (Parallel Operations via SDK)

**Files:**
- Create: `plugins/meta/claude-docs/scripts/jina_mcp_docs.py`
- Test: `plugins/meta/claude-docs/tests/test_jina_mcp_docs.py`
- Reference: `docs/research/web-scraping-methods-comparison.md:116-180`

### Step 1: Write the failing test

```python
#!/usr/bin/env python3
"""Tests for jina_mcp_docs.py script."""

import pytest
from pathlib import Path
from unittest.mock import Mock, patch, AsyncMock
import sys

SCRIPTS_DIR = Path(__file__).parent.parent / "scripts"
sys.path.insert(0, str(SCRIPTS_DIR))

def test_batch_urls_optimal_size():
    """Test that URLs are batched in optimal sizes (3-4 URLs)."""
    from jina_mcp_docs import batch_urls

    urls = [f"https://example.com/page{i}.md" for i in range(10)]
    batches = list(batch_urls(urls, batch_size=3))

    assert len(batches) == 4  # 3 + 3 + 3 + 1
    assert len(batches[0]) == 3
    assert len(batches[1]) == 3
    assert len(batches[2]) == 3
    assert len(batches[3]) == 1

@pytest.mark.asyncio
async def test_sdk_orchestrator_configuration():
    """Test that SDK orchestrator is configured correctly."""
    from jina_mcp_docs import create_orchestrator_options

    options = create_orchestrator_options()

    # Should use claude_code system prompt for orchestrator
    assert options.system_prompt == "claude_code" or \
           (isinstance(options.system_prompt, dict) and
            options.system_prompt.get("preset") == "claude_code")

    # Should have Task tool for delegating to agents
    assert "Task" in options.allowed_tools

@pytest.mark.asyncio
@patch('claude_agent_sdk.ClaudeSDKClient')
async def test_parallel_download_via_mcp(mock_client):
    """Test parallel download orchestration via Jina MCP."""
    from jina_mcp_docs import download_batch_parallel

    # Mock SDK client responses
    mock_instance = AsyncMock()
    mock_client.return_value.__aenter__.return_value = mock_instance

    urls = [
        "https://docs.claude.com/page1.md",
        "https://docs.claude.com/page2.md",
        "https://docs.claude.com/page3.md",
    ]

    results = await download_batch_parallel(urls)

    # Should have attempted parallel download
    assert mock_instance.query.called
    assert len(results) == 3
```

### Step 2: Run test to verify it fails

Run: `pytest plugins/meta/claude-docs/tests/test_jina_mcp_docs.py -v`

Expected: FAIL with "ModuleNotFoundError: No module named 'jina_mcp_docs'"

### Step 3: Write minimal implementation

```python
#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = [
#   "claude-agent-sdk>=0.1.6",
#   "rich>=13.0.0",
#   "typer>=0.12.0",
# ]
# ///
"""
Download Claude Code documentation using Jina MCP Server (parallel operations).

This script demonstrates parallel URL reading via Claude Agent SDK + Jina MCP.
Best for: Research tasks requiring multiple sources, speed optimization (3-4 URLs optimal).

Prerequisites:
    - Jina MCP server configured in Claude settings
    - JINA_API_KEY environment variable set

Usage:
    ./jina_mcp_docs.py                              # Downloads with optimal batching
    ./jina_mcp_docs.py --batch-size 4               # Custom batch size
    ./jina_mcp_docs.py --output-dir docs
"""

import asyncio
import json
import sys
import time
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from typing import AsyncIterator

import typer
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.table import Table

# Claude Agent SDK imports
from claude_agent_sdk import (
    ClaudeSDKClient,
    ClaudeAgentOptions,
    AssistantMessage,
    TextBlock,
)

class OutputFormat(str, Enum):
    """Output format options."""
    RICH = "rich"
    JSON = "json"

console = Console()

@dataclass
class DownloadResult:
    """Result of documentation download operation."""
    status: str
    downloaded: int = 0
    failed: int = 0
    duration_seconds: float = 0.0
    timestamp: str = ""

# Same constants as other scripts
BASE_URL = "https://docs.claude.com/en/docs"
CLAUDE_CODE_BASE = f"{BASE_URL}/claude-code"
AGENTS_TOOLS_BASE = f"{BASE_URL}/agents-and-tools"

CLAUDE_CODE_PAGES = [
    "sub-agents", "plugins", "skills", "output-styles", "hooks-guide",
    "plugin-marketplaces", "settings", "statusline", "slash-commands",
    "hooks", "plugins-reference", "memory", "mcp"
]

AGENT_SKILLS_PAGES = [
    "agent-skills/overview",
    "agent-skills/quickstart",
    "agent-skills/best-practices",
]

DEFAULT_PAGES = [
    ("claude-code", page) for page in CLAUDE_CODE_PAGES
] + [
    ("agents-and-tools", page) for page in AGENT_SKILLS_PAGES
]

# Optimal batch size based on research
OPTIMAL_BATCH_SIZE = 3

def batch_urls(urls: list[str], batch_size: int = OPTIMAL_BATCH_SIZE) -> list[list[str]]:
    """Split URLs into optimal batches for parallel processing."""
    return [urls[i:i + batch_size] for i in range(0, len(urls), batch_size)]

def get_base_url(section: str) -> str:
    """Get the base URL for a documentation section."""
    if section == "claude-code":
        return CLAUDE_CODE_BASE
    elif section == "agents-and-tools":
        return AGENTS_TOOLS_BASE
    else:
        return f"{BASE_URL}/{section}"

def create_orchestrator_options() -> ClaudeAgentOptions:
    """
    Create SDK options for orchestrator that uses Jina MCP tools.

    The orchestrator needs:
    - claude_code system prompt (provides MCP tool knowledge)
    - Jina MCP tools available in environment
    """
    return ClaudeAgentOptions(
        system_prompt="claude_code",  # CRITICAL for orchestrator
        allowed_tools=[
            "mcp__jina-mcp-server__read_url",
            "mcp__jina-mcp-server__parallel_read_url",
        ],
        permission_mode="acceptEdits",
        model="claude-sonnet-4-5-20250929"
    )

async def download_batch_parallel(
    urls: list[str],
    output_dir: Path,
) -> list[tuple[str, bool, str]]:
    """
    Download a batch of URLs in parallel using Jina MCP parallel_read_url.

    Args:
        urls: List of URLs to download (max 3-4 for optimal performance)
        output_dir: Directory to save files

    Returns:
        List of (url, success, content) tuples
    """
    options = create_orchestrator_options()
    results = []

    async with ClaudeSDKClient(options=options) as client:
        # Build parallel read request
        url_objects = [{"url": url} for url in urls]

        # Query Claude to use parallel_read_url MCP tool
        prompt = f"""Use the Jina MCP parallel_read_url tool to fetch these URLs in parallel:

{json.dumps(url_objects, indent=2)}

Return the content for each URL."""

        await client.query(prompt)

        # Collect responses
        full_response = []
        async for message in client.receive_response():
            if isinstance(message, AssistantMessage):
                for block in message.content:
                    if isinstance(block, TextBlock):
                        full_response.append(block.text)

        # Parse response and extract content
        # Note: In production, would parse structured response from MCP tool
        # For now, save the combined response
        combined_content = "\n\n".join(full_response)

        for url in urls:
            # Extract page name from URL
            page_name = url.split("/")[-1].replace(".md", "")
            flat_filename = page_name.replace("/", "-")
            output_file = output_dir / f"{flat_filename}.md"

            # In real implementation, would parse individual responses
            # For demo, save combined output
            output_file.write_text(combined_content)
            results.append((url, True, combined_content))

    return results

async def download_all_parallel(
    pages: list[tuple[str, str]],
    output_dir: Path,
    batch_size: int,
    format: OutputFormat,
) -> tuple[int, int, float, list[float]]:
    """
    Download all pages using parallel batching strategy.

    Returns:
        Tuple of (success_count, failed_count, total_time, batch_times)
    """
    start_time = time.time()
    success_count = 0
    failed_count = 0
    batch_times = []

    # Build full URLs
    urls = []
    for section, page in pages:
        base_url = get_base_url(section)
        urls.append(f"{base_url}/{page}.md")

    # Split into batches
    url_batches = batch_urls(urls, batch_size)

    if format == OutputFormat.RICH:
        console.print(
            f"[cyan]Processing {len(urls)} URLs in {len(url_batches)} batches "
            f"(batch size: {batch_size})[/cyan]\n"
        )

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console,
        ) as progress:
            task = progress.add_task(
                "[cyan]Downloading batches...",
                total=len(url_batches)
            )

            for batch_num, batch in enumerate(url_batches, 1):
                progress.update(
                    task,
                    description=f"[cyan]Batch {batch_num}/{len(url_batches)} "
                    f"({len(batch)} URLs)..."
                )

                batch_start = time.time()
                results = await download_batch_parallel(batch, output_dir)
                batch_duration = time.time() - batch_start
                batch_times.append(batch_duration)

                for url, success, content in results:
                    page_name = url.split("/")[-1].replace(".md", "")
                    if success:
                        console.print(
                            f"[green]âœ“[/green] {page_name} "
                            f"[dim](batch {batch_num}, {len(content)} bytes)[/dim]"
                        )
                        success_count += 1
                    else:
                        console.print(f"[red]âœ—[/red] {page_name}", file=sys.stderr)
                        failed_count += 1

                progress.advance(task)
    else:
        # JSON mode
        for batch in url_batches:
            batch_start = time.time()
            results = await download_batch_parallel(batch, output_dir)
            batch_duration = time.time() - batch_start
            batch_times.append(batch_duration)

            for url, success, content in results:
                if success:
                    success_count += 1
                else:
                    failed_count += 1

    total_time = time.time() - start_time
    return success_count, failed_count, total_time, batch_times

def main(
    output_dir: Path = typer.Option(
        Path("./ai_docs"),
        "--output-dir", "-o",
        help="Directory to save downloaded files",
    ),
    batch_size: int = typer.Option(
        OPTIMAL_BATCH_SIZE,
        "--batch-size",
        help="URLs per parallel batch (3-4 optimal, max 5)",
        min=1, max=5,
    ),
    format: OutputFormat = typer.Option(
        OutputFormat.RICH,
        "--format",
        help="Output format: 'rich' or 'json'",
    ),
) -> None:
    """
    Download Claude Code docs using Jina MCP Server (parallel operations).

    This demonstrates parallel URL reading via Claude Agent SDK.
    Best for: Research tasks, speed optimization (3-4x faster than sequential).

    Requires: Jina MCP server configured with JINA_API_KEY.
    """
    output_dir.mkdir(exist_ok=True, parents=True)

    if format == OutputFormat.RICH:
        console.print(
            "[cyan]Downloading using Jina MCP Server (parallel operations)[/cyan]"
        )
        console.print(f"[dim]Output: {output_dir.absolute()}/[/dim]\n")

    # Run async download
    success_count, failed_count, total_time, batch_times = asyncio.run(
        download_all_parallel(DEFAULT_PAGES, output_dir, batch_size, format)
    )

    # Output results
    if format == OutputFormat.JSON:
        result = {
            "status": "success" if failed_count == 0 else "error",
            "downloaded": success_count,
            "failed": failed_count,
            "duration_seconds": round(total_time, 2),
            "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
        }
        print(json.dumps(result))
    else:
        console.print()
        table = Table(title="Download Summary", show_header=True, header_style="bold cyan")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="green")

        table.add_row("Total Pages", str(len(DEFAULT_PAGES)))
        table.add_row("âœ“ Downloaded", f"[green]{success_count}[/green]")
        if failed_count > 0:
            table.add_row("âœ— Failed", f"[red]{failed_count}[/red]")
        table.add_row("Total Batches", str(len(batch_times)))
        table.add_row("Total Time", f"{total_time:.2f}s")

        if batch_times:
            avg_batch_time = sum(batch_times) / len(batch_times)
            table.add_row("Avg Batch Time", f"{avg_batch_time:.2f}s")

        console.print(table)

        # Performance note
        console.print(
            f"\n[yellow]ðŸ’¡ Performance:[/yellow] Parallel batching "
            f"(~3x faster than sequential). Optimal batch size: 3-4 URLs."
        )

    if failed_count > 0:
        raise typer.Exit(code=1)

if __name__ == "__main__":
    typer.run(main)
```

### Step 4: Run test to verify it passes

Run: `pytest plugins/meta/claude-docs/tests/test_jina_mcp_docs.py -v`

Expected: PASS (all tests green)

### Step 5: Manual integration test

Run: `./plugins/meta/claude-docs/scripts/jina_mcp_docs.py --output-dir /tmp/test-jina-mcp`

Expected: Successfully downloads 17 pages in ~6 batches using parallel operations

### Step 6: Commit

```bash
git add plugins/meta/claude-docs/scripts/jina_mcp_docs.py plugins/meta/claude-docs/tests/test_jina_mcp_docs.py
git commit -m "feat: add jina_mcp_docs.py script for parallel scraping

- Uses Claude Agent SDK with Jina MCP parallel_read_url
- Implements optimal batching (3-4 URLs per batch)
- ~3x faster than sequential processing
- Best for research tasks requiring multiple sources"
```

---

## Task 3: Create firecrawl_mcp_docs.py (Advanced Scraping via SDK)

**Files:**
- Create: `plugins/meta/claude-docs/scripts/firecrawl_mcp_docs.py`
- Test: `plugins/meta/claude-docs/tests/test_firecrawl_mcp_docs.py`
- Reference: `docs/research/web-scraping-methods-comparison.md:201-290`

### Step 1: Write the failing test

```python
#!/usr/bin/env python3
"""Tests for firecrawl_mcp_docs.py script."""

import pytest
from pathlib import Path
from unittest.mock import Mock, patch, AsyncMock
import sys

SCRIPTS_DIR = Path(__file__).parent.parent / "scripts"
sys.path.insert(0, str(SCRIPTS_DIR))

@pytest.mark.asyncio
async def test_firecrawl_orchestrator_configuration():
    """Test that Firecrawl orchestrator is configured correctly."""
    from firecrawl_mcp_docs import create_orchestrator_options

    options = create_orchestrator_options()

    # Should use claude_code system prompt
    assert options.system_prompt == "claude_code" or \
           (isinstance(options.system_prompt, dict) and
            options.system_prompt.get("preset") == "claude_code")

    # Should have Firecrawl MCP tools
    assert "mcp__firecrawl__firecrawl_scrape" in options.allowed_tools

@pytest.mark.asyncio
@patch('claude_agent_sdk.ClaudeSDKClient')
async def test_download_with_metadata(mock_client):
    """Test that Firecrawl returns rich metadata."""
    from firecrawl_mcp_docs import download_page_firecrawl

    mock_instance = AsyncMock()
    mock_client.return_value.__aenter__.return_value = mock_instance

    url = "https://docs.claude.com/page1.md"
    success, content, metadata = await download_page_firecrawl(url, Path("/tmp"))

    # Should have attempted download via MCP
    assert mock_instance.query.called

@pytest.mark.asyncio
async def test_error_handling_robustness():
    """Test that Firecrawl handles errors gracefully."""
    from firecrawl_mcp_docs import download_page_firecrawl

    # Should handle failures without crashing
    url = "https://invalid-url-that-will-fail.com"
    success, content, metadata = await download_page_firecrawl(url, Path("/tmp"))

    # May succeed or fail, but should not raise exception
    assert isinstance(success, bool)
```

### Step 2: Run test to verify it fails

Run: `pytest plugins/meta/claude-docs/tests/test_firecrawl_mcp_docs.py -v`

Expected: FAIL with "ModuleNotFoundError: No module named 'firecrawl_mcp_docs'"

### Step 3: Write minimal implementation

```python
#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = [
#   "claude-agent-sdk>=0.1.6",
#   "rich>=13.0.0",
#   "typer>=0.12.0",
# ]
# ///
"""
Download Claude Code documentation using Firecrawl MCP Server (robust scraping).

This script demonstrates production-grade scraping via Claude Agent SDK + Firecrawl MCP.
Best for: Production scraping, complex pages, need for rich metadata and reliability.

Prerequisites:
    - Firecrawl MCP server configured in Claude settings
    - FIRECRAWL_API_KEY environment variable set

Usage:
    ./firecrawl_mcp_docs.py                         # Downloads with full metadata
    ./firecrawl_mcp_docs.py --output-dir docs
    ./firecrawl_mcp_docs.py --main-content-only     # Strip navigation/footers
"""

import asyncio
import json
import sys
import time
from dataclasses import dataclass
from enum import Enum
from pathlib import Path

import typer
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.table import Table

from claude_agent_sdk import (
    ClaudeSDKClient,
    ClaudeAgentOptions,
    AssistantMessage,
    TextBlock,
)

class OutputFormat(str, Enum):
    """Output format options."""
    RICH = "rich"
    JSON = "json"

console = Console()

@dataclass
class DownloadResult:
    """Result of documentation download operation."""
    status: str
    downloaded: int = 0
    failed: int = 0
    duration_seconds: float = 0.0
    timestamp: str = ""

# Same constants
BASE_URL = "https://docs.claude.com/en/docs"
CLAUDE_CODE_BASE = f"{BASE_URL}/claude-code"
AGENTS_TOOLS_BASE = f"{BASE_URL}/agents-and-tools"

CLAUDE_CODE_PAGES = [
    "sub-agents", "plugins", "skills", "output-styles", "hooks-guide",
    "plugin-marketplaces", "settings", "statusline", "slash-commands",
    "hooks", "plugins-reference", "memory", "mcp"
]

AGENT_SKILLS_PAGES = [
    "agent-skills/overview",
    "agent-skills/quickstart",
    "agent-skills/best-practices",
]

DEFAULT_PAGES = [
    ("claude-code", page) for page in CLAUDE_CODE_PAGES
] + [
    ("agents-and-tools", page) for page in AGENT_SKILLS_PAGES
]

def get_base_url(section: str) -> str:
    """Get the base URL for a documentation section."""
    if section == "claude-code":
        return CLAUDE_CODE_BASE
    elif section == "agents-and-tools":
        return AGENTS_TOOLS_BASE
    else:
        return f"{BASE_URL}/{section}"

def create_orchestrator_options() -> ClaudeAgentOptions:
    """
    Create SDK options for orchestrator that uses Firecrawl MCP tools.

    Firecrawl provides:
    - Robust scraping with better error handling
    - Rich metadata (cache status, credits used, etc.)
    - Support for complex/large pages
    """
    return ClaudeAgentOptions(
        system_prompt="claude_code",
        allowed_tools=[
            "mcp__firecrawl__firecrawl_scrape",
            "mcp__firecrawl__firecrawl_search",
        ],
        permission_mode="acceptEdits",
        model="claude-sonnet-4-5-20250929"
    )

async def download_page_firecrawl(
    url: str,
    output_dir: Path,
    main_content_only: bool = True,
) -> tuple[bool, str, dict]:
    """
    Download a page using Firecrawl MCP scrape tool.

    Args:
        url: Full URL to download
        output_dir: Directory to save file
        main_content_only: Strip navigation/footers (default: True)

    Returns:
        Tuple of (success, content, metadata)
    """
    options = create_orchestrator_options()

    async with ClaudeSDKClient(options=options) as client:
        # Build scrape request
        prompt = f"""Use the Firecrawl firecrawl_scrape tool to fetch this URL:

URL: {url}
Options:
- formats: ["markdown"]
- onlyMainContent: {str(main_content_only).lower()}

Return the scraped markdown content and metadata."""

        await client.query(prompt)

        # Collect response
        full_response = []
        async for message in client.receive_response():
            if isinstance(message, AssistantMessage):
                for block in message.content:
                    if isinstance(block, TextBlock):
                        full_response.append(block.text)

        combined_content = "\n\n".join(full_response)

        # Extract page name and save
        page_name = url.split("/")[-1].replace(".md", "")
        flat_filename = page_name.replace("/", "-")
        output_file = output_dir / f"{flat_filename}.md"
        output_file.write_text(combined_content)

        # Metadata (in production, would parse from MCP tool response)
        metadata = {
            "url": url,
            "size": len(combined_content),
            "main_content_only": main_content_only,
        }

        return True, combined_content, metadata

async def download_all_sequential(
    pages: list[tuple[str, str]],
    output_dir: Path,
    main_content_only: bool,
    format: OutputFormat,
) -> tuple[int, int, float]:
    """
    Download all pages sequentially using Firecrawl.

    Note: Firecrawl doesn't support parallel operations,
    but provides better reliability for individual pages.

    Returns:
        Tuple of (success_count, failed_count, total_time)
    """
    start_time = time.time()
    success_count = 0
    failed_count = 0

    if format == OutputFormat.RICH:
        console.print(
            f"[cyan]Processing {len(pages)} URLs sequentially "
            f"(Firecrawl optimized for reliability)[/cyan]\n"
        )

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console,
        ) as progress:
            task = progress.add_task(
                "[cyan]Downloading...",
                total=len(pages)
            )

            for section, page in pages:
                base_url = get_base_url(section)
                url = f"{base_url}/{page}.md"

                progress.update(task, description=f"[cyan]Downloading {page}...")

                page_start = time.time()
                try:
                    success, content, metadata = await download_page_firecrawl(
                        url, output_dir, main_content_only
                    )
                    duration = time.time() - page_start

                    if success:
                        size_kb = metadata["size"] / 1024
                        console.print(
                            f"[green]âœ“[/green] {page} "
                            f"[dim]({size_kb:.1f}KB in {duration:.2f}s)[/dim]"
                        )
                        success_count += 1
                    else:
                        console.print(f"[red]âœ—[/red] {page}", file=sys.stderr)
                        failed_count += 1
                except Exception as e:
                    console.print(
                        f"[red]âœ—[/red] {page}: {e}",
                        file=sys.stderr
                    )
                    failed_count += 1

                progress.advance(task)
    else:
        # JSON mode
        for section, page in pages:
            base_url = get_base_url(section)
            url = f"{base_url}/{page}.md"

            try:
                success, content, metadata = await download_page_firecrawl(
                    url, output_dir, main_content_only
                )
                if success:
                    success_count += 1
                else:
                    failed_count += 1
            except Exception:
                failed_count += 1

    total_time = time.time() - start_time
    return success_count, failed_count, total_time

def main(
    output_dir: Path = typer.Option(
        Path("./ai_docs"),
        "--output-dir", "-o",
        help="Directory to save downloaded files",
    ),
    main_content_only: bool = typer.Option(
        True,
        "--main-content-only",
        help="Extract only main content (strip navigation/footers)",
    ),
    format: OutputFormat = typer.Option(
        OutputFormat.RICH,
        "--format",
        help="Output format: 'rich' or 'json'",
    ),
) -> None:
    """
    Download Claude Code docs using Firecrawl MCP Server (robust scraping).

    This demonstrates production-grade scraping with rich metadata.
    Best for: Complex pages, need for reliability, production scraping.

    Requires: Firecrawl MCP server configured with FIRECRAWL_API_KEY.
    """
    output_dir.mkdir(exist_ok=True, parents=True)

    if format == OutputFormat.RICH:
        console.print(
            "[cyan]Downloading using Firecrawl MCP Server (robust scraping)[/cyan]"
        )
        console.print(f"[dim]Output: {output_dir.absolute()}/[/dim]\n")

    # Run async download
    success_count, failed_count, total_time = asyncio.run(
        download_all_sequential(DEFAULT_PAGES, output_dir, main_content_only, format)
    )

    # Output results
    if format == OutputFormat.JSON:
        result = {
            "status": "success" if failed_count == 0 else "error",
            "downloaded": success_count,
            "failed": failed_count,
            "duration_seconds": round(total_time, 2),
            "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
        }
        print(json.dumps(result))
    else:
        console.print()
        table = Table(title="Download Summary", show_header=True, header_style="bold cyan")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="green")

        table.add_row("Total Pages", str(len(DEFAULT_PAGES)))
        table.add_row("âœ“ Downloaded", f"[green]{success_count}[/green]")
        if failed_count > 0:
            table.add_row("âœ— Failed", f"[red]{failed_count}[/red]")
        table.add_row("Total Time", f"{total_time:.2f}s")

        console.print(table)

        # Feature note
        console.print(
            f"\n[yellow]ðŸ’¡ Features:[/yellow] Rich metadata, robust error handling, "
            f"complex page support. Trade speed for reliability."
        )

    if failed_count > 0:
        raise typer.Exit(code=1)

if __name__ == "__main__":
    typer.run(main)
```

### Step 4: Run test to verify it passes

Run: `pytest plugins/meta/claude-docs/tests/test_firecrawl_mcp_docs.py -v`

Expected: PASS (all tests green)

### Step 5: Manual integration test

Run: `./plugins/meta/claude-docs/scripts/firecrawl_mcp_docs.py --output-dir /tmp/test-firecrawl`

Expected: Successfully downloads 17 pages with rich metadata

### Step 6: Commit

```bash
git add plugins/meta/claude-docs/scripts/firecrawl_mcp_docs.py plugins/meta/claude-docs/tests/test_firecrawl_mcp_docs.py
git commit -m "feat: add firecrawl_mcp_docs.py script for robust scraping

- Uses Claude Agent SDK with Firecrawl MCP scrape tool
- Implements production-grade scraping with rich metadata
- Better error handling for complex/large pages
- Best for reliability over speed"
```

---

## Task 4: Create Supporting Documentation

**Files:**
- Create: `plugins/meta/claude-docs/docs/script-comparison.md`
- Reference: `docs/research/web-scraping-methods-comparison.md`

### Step 1: Write documentation outline

Create documentation that helps users choose the right script:

```markdown
# Claude Docs Script Comparison

> Guide to choosing the right script for downloading Claude Code documentation

## Available Scripts

1. **jina_reader_docs.py** - Direct HTTP calls via Jina Reader API
2. **jina_mcp_docs.py** - Parallel operations via Jina MCP Server + SDK
3. **firecrawl_mcp_docs.py** - Robust scraping via Firecrawl MCP + SDK

## Quick Decision Guide

**Need speed for multiple pages?** â†’ Use `jina_mcp_docs.py` (parallel batching, ~3x faster)

**Need simplicity and no MCP setup?** â†’ Use `jina_reader_docs.py` (direct HTTP, works anywhere)

**Need production reliability?** â†’ Use `firecrawl_mcp_docs.py` (robust error handling, rich metadata)

---

## Detailed Comparison

### jina_reader_docs.py - Direct HTTP Calls

**Method:** Direct HTTP requests to Jina Reader API using `requests` library

**Best for:**
- Scripts and automation without MCP setup
- Simple one-off downloads
- Environments where MCP is not available
- Direct control over HTTP requests

**Pros:**
- âœ… Simple setup (just needs `requests` library)
- âœ… Works anywhere (no MCP required)
- âœ… Free tier available (20 RPM without API key)
- âœ… Transparent HTTP control

**Cons:**
- âŒ Sequential only (no parallel operations)
- âŒ Slower for multiple pages
- âŒ Manual error handling required

**Prerequisites:**
- None (optional: JINA_API_KEY for higher rate limits)

**Usage:**
```bash
# Free tier (20 RPM)
./jina_reader_docs.py

# With API key (500 RPM)
export JINA_API_KEY="your-key"
./jina_reader_docs.py

# Custom output directory
./jina_reader_docs.py --output-dir ./docs
```

**Performance:**
- Single page: ~2-8 seconds
- 17 pages sequential: ~60-120 seconds
- Rate limits: 20 RPM (free) / 500 RPM (with key)

---

### jina_mcp_docs.py - Parallel Operations

**Method:** Claude Agent SDK orchestrating Jina MCP `parallel_read_url` tool

**Best for:**
- Research tasks requiring multiple sources
- Speed optimization (3-4x faster than sequential)
- Batch processing 3-4 URLs at a time
- MCP-enabled environments

**Pros:**
- âœ… Fast (parallel operations)
- âœ… Optimal batching (3-4 URLs per batch)
- âœ… ~3x faster than sequential
- âœ… SDK orchestration patterns

**Cons:**
- âŒ Requires MCP server setup
- âŒ Timeout risk with >5 URLs per batch
- âŒ More complex (SDK + async)

**Prerequisites:**
- Jina MCP server configured in Claude settings
- JINA_API_KEY environment variable
- claude-agent-sdk>=0.1.6

**Usage:**
```bash
# Default (optimal batch size: 3)
./jina_mcp_docs.py

# Custom batch size
./jina_mcp_docs.py --batch-size 4

# Custom output directory
./jina_mcp_docs.py --output-dir ./docs
```

**Performance:**
- Batch of 3 URLs: ~8-12 seconds (vs 24 seconds sequential)
- 17 pages in 6 batches: ~40-60 seconds
- **~3x faster** than sequential
- Optimal batch size: 3-4 URLs

---

### firecrawl_mcp_docs.py - Robust Scraping

**Method:** Claude Agent SDK using Firecrawl MCP `firecrawl_scrape` tool

**Best for:**
- Production web scraping
- Complex or large pages
- Need for rich metadata
- Reliability over speed

**Pros:**
- âœ… Robust (handles edge cases well)
- âœ… Rich metadata (cache status, credits, etc.)
- âœ… Better error handling
- âœ… Main content extraction

**Cons:**
- âŒ Sequential only (no parallel operations)
- âŒ Slower than parallel methods
- âŒ Uses Firecrawl credits
- âŒ Requires MCP setup

**Prerequisites:**
- Firecrawl MCP server configured in Claude settings
- FIRECRAWL_API_KEY environment variable
- claude-agent-sdk>=0.1.6

**Usage:**
```bash
# Default (main content only)
./firecrawl_mcp_docs.py

# Include navigation/footers
./firecrawl_mcp_docs.py --no-main-content-only

# Custom output directory
./firecrawl_mcp_docs.py --output-dir ./docs
```

**Performance:**
- Single page: ~3-10 seconds
- 17 pages sequential: ~60-150 seconds
- More reliable than faster methods
- Better for complex pages

---

## Performance Comparison

| Script | 17 Pages | Speed | Complexity | Reliability |
|--------|----------|-------|------------|-------------|
| jina_reader_docs.py | ~60-120s | Slow | Low | Good |
| jina_mcp_docs.py | ~40-60s | **Fast** | Medium | Good |
| firecrawl_mcp_docs.py | ~60-150s | Slow | Medium | **Excellent** |

**Winner for speed:** jina_mcp_docs.py (parallel batching)
**Winner for simplicity:** jina_reader_docs.py (no MCP setup)
**Winner for reliability:** firecrawl_mcp_docs.py (robust error handling)

---

## Use Case Examples

### Quick One-Time Download
**Recommended:** `jina_reader_docs.py`

```bash
./jina_reader_docs.py --output-dir ./temp-docs
```

**Why:** Simple, no setup, good enough for one-time use.

---

### Daily Documentation Sync
**Recommended:** `jina_mcp_docs.py`

```bash
./jina_mcp_docs.py --output-dir ~/docs/claude --batch-size 3
```

**Why:** Fast parallel downloads save time on regular updates.

---

### Production Documentation Pipeline
**Recommended:** `firecrawl_mcp_docs.py`

```bash
./firecrawl_mcp_docs.py --output-dir /var/docs/claude --main-content-only
```

**Why:** Reliable, handles edge cases, rich metadata for monitoring.

---

## Common Options (All Scripts)

All three scripts support:

- `--output-dir, -o` - Custom output directory
- `--format` - Output format (`rich` or `json`)
- `--retries, -r` - Retry attempts (jina_reader_docs.py only)
- `--batch-size` - Batch size (jina_mcp_docs.py only)
- `--main-content-only` - Content extraction (firecrawl_mcp_docs.py only)

---

## API Key Setup

### Jina Reader API

```bash
# Free tier (20 RPM)
# No setup needed

# With API key (500 RPM)
export JINA_API_KEY="your-jina-api-key"
```

### Firecrawl API

```bash
export FIRECRAWL_API_KEY="your-firecrawl-api-key"
```

---

## Troubleshooting

### "Module not found: claude_agent_sdk"

**Scripts affected:** jina_mcp_docs.py, firecrawl_mcp_docs.py

**Solution:** Install the SDK:
```bash
pip install claude-agent-sdk>=0.1.6
```

### "MCP server not configured"

**Scripts affected:** jina_mcp_docs.py, firecrawl_mcp_docs.py

**Solution:** Configure MCP servers in Claude settings:
```json
{
  "mcpServers": {
    "jina-mcp-server": {
      "command": "npx",
      "args": ["-y", "mcp-remote", "https://mcp.jina.ai/sse"],
      "env": {"JINA_API_KEY": "your-key"}
    },
    "firecrawl": {
      "command": "npx",
      "args": ["-y", "firecrawl-mcp"],
      "env": {"FIRECRAWL_API_KEY": "your-key"}
    }
  }
}
```

### Rate limit errors (429)

**Scripts affected:** jina_reader_docs.py

**Solution:** Use API key or reduce request rate:
```bash
export JINA_API_KEY="your-key"  # Increases limit to 500 RPM
```

### Timeout errors

**Scripts affected:** jina_mcp_docs.py

**Solution:** Reduce batch size:
```bash
./jina_mcp_docs.py --batch-size 3  # Safer than 5
```

---

## Summary

**Choose based on your priorities:**

- **Speed** â†’ jina_mcp_docs.py (parallel batching)
- **Simplicity** â†’ jina_reader_docs.py (no MCP setup)
- **Reliability** â†’ firecrawl_mcp_docs.py (robust scraping)

**All scripts successfully download Claude Code documentation.**
The choice depends on your environment, performance needs, and reliability requirements.
```bash

### Step 2: Review documentation for clarity

Run: `cat plugins/meta/claude-docs/docs/script-comparison.md | head -50`

Expected: Clear, well-structured comparison guide

### Step 3: Commit documentation

```bash
git add plugins/meta/claude-docs/docs/script-comparison.md
git commit -m "docs: add script comparison guide for claude_docs variations

- Compares all three scraping approaches
- Includes decision guide and use cases
- Documents setup, troubleshooting, performance
- Helps users choose the right script"
```

---

## Task 5: Update Main README

**Files:**
- Modify: `plugins/meta/claude-docs/README.md`

### Step 1: Add scripts section to README

```markdown
## Available Scripts

The `scripts/` directory contains multiple implementations for downloading Claude Code documentation, each using a different web scraping approach:

### claude_docs.py (Original)
Direct HTTP downloads using `httpx` with caching and incremental updates.

**Best for:** Standard usage, incremental updates, caching

```bash
./scripts/claude_docs.py --output-dir ./ai_docs
```

### jina_reader_docs.py
Direct Jina Reader API calls using `requests` library.

**Best for:** Simple scripts, no MCP setup, direct HTTP control

```bash
./scripts/jina_reader_docs.py --output-dir ./ai_docs
```

### jina_mcp_docs.py
Parallel operations via Claude Agent SDK + Jina MCP Server.

**Best for:** Speed (3x faster), research tasks, batch processing

```bash
./scripts/jina_mcp_docs.py --batch-size 3 --output-dir ./ai_docs
```

### firecrawl_mcp_docs.py
Robust scraping via Claude Agent SDK + Firecrawl MCP Server.

**Best for:** Production reliability, complex pages, rich metadata

```bash
./scripts/firecrawl_mcp_docs.py --main-content-only --output-dir ./ai_docs
```

**For detailed comparison and decision guide, see:** [docs/script-comparison.md](docs/script-comparison.md)
```sql

### Step 2: Verify README formatting

Run: `cat plugins/meta/claude-docs/README.md | grep -A 30 "Available Scripts"`

Expected: Well-formatted scripts section with clear descriptions

### Step 3: Commit README update

```bash
git add plugins/meta/claude-docs/README.md
git commit -m "docs: update README with script variations section

- Documents all four scraping scripts
- Provides quick comparison and use cases
- Links to detailed comparison guide"
```

---

## Task 6: Verify All Scripts Work

**Files:**
- Test: All three new scripts

### Step 1: Test jina_reader_docs.py

Run:
```bash
cd plugins/meta/claude-docs
./scripts/jina_reader_docs.py --output-dir /tmp/test-jina-reader --format json
```

Expected: JSON output showing successful downloads

### Step 2: Test jina_mcp_docs.py

Run:
```bash
cd plugins/meta/claude-docs
./scripts/jina_mcp_docs.py --output-dir /tmp/test-jina-mcp --batch-size 3 --format json
```

Expected: JSON output showing parallel batch downloads

### Step 3: Test firecrawl_mcp_docs.py

Run:
```bash
cd plugins/meta/claude-docs
./scripts/firecrawl_mcp_docs.py --output-dir /tmp/test-firecrawl --format json
```

Expected: JSON output showing robust scraping with metadata

### Step 4: Run all tests

Run: `pytest plugins/meta/claude-docs/tests/ -v`

Expected: All tests pass

### Step 5: Final commit

```bash
git add -A
git commit -m "test: verify all script variations work end-to-end

- jina_reader_docs.py: direct HTTP âœ“
- jina_mcp_docs.py: parallel operations âœ“
- firecrawl_mcp_docs.py: robust scraping âœ“
- All tests passing"
```

---

## Completion Checklist

- [ ] Task 1: jina_reader_docs.py created and tested
- [ ] Task 2: jina_mcp_docs.py created and tested
- [ ] Task 3: firecrawl_mcp_docs.py created and tested
- [ ] Task 4: Supporting documentation created
- [ ] Task 5: README updated
- [ ] Task 6: All scripts verified working
- [ ] All tests passing
- [ ] All commits made with descriptive messages

---

## Success Criteria Met

âœ… **3 standalone scripts** demonstrating different scraping approaches
âœ… **Each script demonstrates its method's strengths** (speed, simplicity, reliability)
âœ… **Supporting documentation** helps users choose the right script
âœ… **Best practices applied** from research and SDK patterns
âœ… **TDD approach** with tests for each script
âœ… **Complete documentation** for troubleshooting and usage

---

## Execution Handoff

Plan complete and saved to `docs/plans/claude-docs-upgrade/challenge-plan2.md`.

**Two execution options:**

**1. Subagent-Driven (this session)**
- I dispatch fresh subagent per task
- Review between tasks
- Fast iteration

**2. Parallel Session (separate)**
- Open new session with executing-plans
- Batch execution with checkpoints

**Which approach?**
