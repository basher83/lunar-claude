# GitOps Bootstrap Specification
---
# INITIATIVE METADATA
initiative:
  name: "gitops-bootstrap"
  started: "2026-01-04"
  status: "phase-5-complete"
  repository: "mothership-gitops"
  depends_on: "omni-deployment (production-cluster-operational)"
  summary: |
    GitOps foundation for talos-prod-01 cluster.
    Single kubectl apply bootstrap, everything else via ArgoCD.
    App of Apps pattern with sync waves for ordered deployment.

---
# DEFINITION OF DONE
definition_of_done:
  - "Single bootstrap command deploys entire stack"
  - "All components managed via ArgoCD Applications"
  - "Secrets sourced from Infisical via ESO"
  - "Services accessible via Tailscale (no public exposure)"
  - "Persistent storage available via Longhorn"
  - "Cluster monitoring operational via Netdata"
  - "Homelab dashboard operational via Homarr"
  - "ArgoCD running in HA mode with persistent storage"
  - "Recovery procedure tested (destroy/recreate drill)"
  - "Longhorn backups configured to S3-compatible storage"

---
# ARCHITECTURE DECISIONS (Locked)

locked_decisions:
  repository_structure:
    decision: "Separate GitOps repo (mothership-gitops)"
    rationale:
      - "Omni-Scale = how cluster exists (infra provisioning)"
      - "mothership-gitops = what runs on cluster (workloads)"
      - "Different change cadences, clean git history"
    change_cost: "low - repo is new"

  bootstrap_pattern:
    decision: "App of Apps with sync waves"
    rationale:
      - "Single entry point (bootstrap.yaml)"
      - "Ordered deployment via sync waves"
      - "Self-managing ArgoCD upgrades"
    reference: "https://argo-cd.readthedocs.io/en/stable/operator-manual/cluster-bootstrapping/"

  secrets_bootstrap:
    decision: "One manual secret for Infisical auth, everything else via ESO"
    rationale: "Breaks chicken-and-egg cleanly with single documented manual step"
    manual_secret:
      name: "universal-auth-credentials"
      namespace: "external-secrets"

  longhorn_replicas:
    decision: "2 replicas (workers only)"
    rationale: "Control planes tainted NoSchedule, originally 2 workers = max 2 replicas"
    reconsider_after: "phase_6 (backups in place)"
    note: "Now have 3 workers - can increase to 3 replicas for better redundancy"

  argocd_ha_strategy:
    decision: "Bootstrap non-HA, self-upgrade to HA after Longhorn"
    rationale: "Non-HA needs no PVCs; HA requires Redis + persistent storage"
    pattern: "sync-wave 99 + manual sync policy"

  infisical_path_scoping:
    decision: "Separate ClusterSecretStore per Infisical path"
    rationale: "Single store with secretsPath only works for one path"
    stores: ["infisical-tailscale", "infisical-netdata", "infisical-homarr"]

  tailscale_exposure:
    decision: "Use Ingress (ingressClassName: tailscale) for service exposure"
    rationale:
      - "More control than Service annotations (hostname, TLS, path routing)"
      - "Consistent pattern across all exposed services"
    status: "implemented"
    services:
      - "argocd.tailfb3ea.ts.net"
      - "longhorn.tailfb3ea.ts.net"
      - "homarr.tailfb3ea.ts.net"
      - "netdata.tailfb3ea.ts.net"
    pattern: |
      apiVersion: networking.k8s.io/v1
      kind: Ingress
      spec:
        ingressClassName: tailscale

  tailnet_first_class_citizen:
    decision: "Cluster as first-class tailnet citizen via DNSConfig + CoreDNS SSA"
    rationale:
      - "Pods can initiate connections to tailnet services with valid TLS"
      - "Apps use native tailnet FQDNs (no K8s Service name indirection)"
      - "Architectural consistency - cluster behaves like any tailnet client"
    trade_offs:
      - "2-5 min DNS gap for *.ts.net after Talos upgrades until ArgoCD reconciles"
      - "Operational dependency on ArgoCD for CoreDNS config persistence"
    implementation:
      - "DNSConfig CR provides k8s-nameserver (manages egress/ingress host records)"
      - "CoreDNS ConfigMap patch with ArgoCD SSA for persistence"
      - "Forward ts.net and tailfb3ea.ts.net to nameserver"
      - "Egress Services required for connectivity (transparent when apps use FQDN)"
    status: "implemented"
    discovered: "2026-01-17"
    key_insight: |
      Nameserver only resolves operator-managed hosts (egress/ingress), not full MagicDNS.
      Egress Services remain required but are transparent: when pods connect via native FQDN,
      DNS returns egress proxy IP, but TLS handshake uses correct SNI (the FQDN), so
      Tailscale Serve accepts the connection. The issue was connecting via K8s Service name
      (wrong SNI), not egress proxies themselves.
    proposal: "proposals/Proposal-Kubernetes-Tailnet-First-Class-Citizen.md"

---
# CONSTRAINTS (Lessons Learned)

constraints:
  single_manual_secret:
    description: "One kubectl create secret for Infisical auth"
    impact: "Must recreate manually after cluster rebuild"

  argocd_ha_manual_sync:
    description: "HA upgrade requires manual trigger"
    rationale: "Safety gate - ensure Longhorn healthy first"

  talos_longhorn_disk_config:
    description: "Longhorn disks must be manually configured on Talos nodes"
    resolution: "Patch nodes.longhorn.io with default-disk config after install"

  pod_security_privileged:
    description: "Longhorn, Netdata, Tailscale namespaces require privileged PSA"
    resolution: "Add label pod-security.kubernetes.io/enforce=privileged"

  eso_default_fields:
    description: "ESO adds default fields to ExternalSecrets causing ArgoCD drift"
    resolution: "Use ignoreDifferences in parent Applications"

  netdata_claiming_via_env:
    description: "Netdata claiming requires envFrom, not inline Helm values"
    resolution: "Use envFrom.secretRef to inject from ESO-managed secret"

  netdata_k8sstate_config_structure:
    description: "Netdata k8sState.configs entries require enabled/path/data structure"
    symptom: "helm template error: cannot overwrite table with non table"
    resolution: "Each config entry must be object with enabled, path, data fields"
    discovered: "2026-01-10"

  redis_ha_topology_mismatch:
    description: "Redis HA 3rd replica stuck Pending with 2 workers"
    cause: "Anti-affinity requires 3 schedulable nodes, only 2 workers available"
    resolution: "Add 3rd worker via Omni"
    status: "resolved"
    resolved: "2026-01-08"

  tailscale_ingress_for_https:
    description: "Service annotations don't propagate backend port in some charts"
    resolution: "Use Tailscale Ingress (ingressClassName: tailscale) for HTTPS on 443"

  web_ui_exposure_required:
    description: "Any application with a web UI MUST be exposed via Tailscale Ingress"
    severity: "hard-requirement"
    rationale: "No web UIs should be cluster-internal only - defeats purpose of homelab"
    pattern: |
      apiVersion: networking.k8s.io/v1
      kind: Ingress
      metadata:
        name: <app>-tailscale
        namespace: <app-namespace>
      spec:
        ingressClassName: tailscale
        rules:
          - http:
              paths:
                - path: /
                  pathType: Prefix
                  backend:
                    service:
                      name: <frontend-service>
                      port:
                        number: 80

  # DR Drill Round 1 (2026-01-14) - Bootstrap Sequence Constraints

  cni_before_argocd:
    description: "CNI must be installed via Helm BEFORE ArgoCD"
    severity: "hard-requirement"
    rationale: "GitOps-managed CNI creates scheduling deadlock - ArgoCD pods need CNI to schedule, CNI needs ArgoCD to deploy"
    resolution: "Install Cilium via Helm directly, wait for nodes Ready, then install ArgoCD"
    discovered: "2026-01-14"

  eso_crd_ordering:
    description: "ESO operator must be installed BEFORE ESO app syncs"
    severity: "hard-requirement"
    rationale: "ESO app contains ClusterSecretStore CRs - CRDs must exist first"
    resolution: "Install ESO operator via Helm with installCRDs=true before app-of-apps"
    discovered: "2026-01-14"

  longhorn_no_hooks_fresh_install:
    description: "Fresh Longhorn install requires --no-hooks flag"
    severity: "hard-requirement"
    rationale: "Helm pre-upgrade hooks require ServiceAccount that chart creates - chicken-and-egg on fresh install"
    resolution: "helm install longhorn ... --no-hooks (subsequent upgrades can use hooks)"
    discovered: "2026-01-14"

  argocd_helm_not_raw_manifests:
    description: "ArgoCD must be installed via Helm, not raw manifests"
    severity: "hard-requirement"
    rationale: |
      Raw manifests have multiple issues:
      - NetworkPolicies block internal traffic
      - Label mismatches break endpoint resolution
      - Port naming mismatches break EndpointSlice
    resolution: "helm install argocd argo/argo-cd --set global.networkPolicy.create=false"
    discovered: "2026-01-14"

  homarr_persistence_config_structure:
    description: "Homarr Helm chart uses persistence.homarrDatabase, not persistence.database"
    symptom: "No PVC created, SQLite stored in ephemeral container storage"
    cause: "Chart uses non-standard persistence key; incorrect values silently ignored"
    resolution: "Use persistence.homarrDatabase.enabled - see mothership-gitops CLAUDE.md"
    discovered: "2026-01-16"

  argocd_redis_ha_persistence:
    description: "ArgoCD redis-ha subchart defaults to emptyDir, not persistent storage"
    symptom: "Redis data lost on pod restart"
    cause: "redis-ha.persistentVolume.enabled defaults to false"
    resolution: "Enable redis-ha.persistentVolume - see mothership-gitops CLAUDE.md"
    discovered: "2026-01-16"

  statefulset_volumeclaimtemplates_immutable:
    description: "K8s StatefulSet volumeClaimTemplates cannot be added after creation"
    symptom: "ArgoCD sync succeeds but StatefulSet shows Missing after adding PVC config"
    cause: "Kubernetes rejects volumeClaimTemplates changes on existing StatefulSets"
    resolution: "Delete StatefulSet manually, then sync again (ArgoCD sync won't recreate)"
    discovered: "2026-01-16"

  coredns_talos_volatile:
    description: "CoreDNS ConfigMap on Talos may be overwritten on upgrade/reboot"
    symptom: "Custom DNS forwarding rules disappear after Talos maintenance"
    cause: "Talos manages CoreDNS ConfigMap; manual patches are not persistent"
    resolution: "Reapply CoreDNS patch after Talos upgrades, or use Talos machine config for DNS customization"
    discovered: "2026-01-16"

  tailscale_egress_sni:
    description: "Tailscale operator egress Services require DNSConfig for valid TLS"
    symptom: "TLS errors (alert 80) when connecting to Tailscale Serve endpoints via egress"
    cause: "Egress proxy sends K8s service name as SNI; Tailscale Serve expects tailnet FQDN"
    resolution: "Deploy DNSConfig CR + CoreDNS stub resolver; pods connect using actual FQDN with correct SNI"
    discovered: "2026-01-16"

  cilium_talos_configuration:
    description: "Cilium on Talos requires specific configuration"
    severity: "hard-requirement"
    rationale: "Talos disables kube-proxy and uses cgroupv2 with specific mount paths"
    required_values:
      kubeProxyReplacement: true # Talos has no kube-proxy
      k8sServiceHost: "<control-plane-ip>" # Must be actual API IP, not ClusterIP
      k8sServicePort: 6443
      cgroup:
        autoMount:
          enabled: false
        hostRoot: "/sys/fs/cgroup"
    discovered: "2026-01-15"

  tailscale_egress_sni_limitation:
    description: "Connecting to egress Services via K8s Service name causes SNI mismatch"
    symptom: "TLS errors (alert 80) connecting to Tailscale Serve endpoints"
    cause: "Client sets SNI to K8s Service name; Tailscale Serve expects tailnet FQDN"
    resolution: "Connect via native FQDN, not K8s Service name. Egress proxy is transparent when FQDN used."
    see_also: "tailnet_first_class_citizen.key_insight"
    discovered: "2026-01-16"

  rwo_single_replica_strategy:
    description: "Single-replica deployments with RWO volumes need Recreate strategy"
    symptom: "Rolling update stuck—new pod can't mount volume attached to old pod on different node"
    resolution: "Set strategy.type: Recreate in Helm values or deployment spec"
    applies_to:
      - homarr (fixed)
      - netdata-parent (chart doesn't expose strategy)
      - netdata-k8s-state (chart doesn't expose strategy)
    discovered: "2026-01-17"

---
# PHASES

phases:
  phase_1:
    name: "Repository Setup"
    status: "complete"
    completed: "2026-01-04"

  phase_2:
    name: "Core Infrastructure Apps"
    status: "complete"
    completed: "2026-01-04"
    components: ["ArgoCD", "External Secrets Operator", "ClusterSecretStores"]

  phase_3:
    name: "Platform Services"
    status: "complete"
    completed: "2026-01-04"
    components: ["Tailscale Operator", "Longhorn", "Netdata"]

  phase_4:
    name: "ArgoCD HA Upgrade"
    status: "complete"
    completed: "2026-01-08"

  phase_5:
    name: "Validation & Documentation"
    status: "complete"
    completed: "2026-01-15"
    tasks:
      - "Run recovery drill: destroy cluster via Omni, recreate from spec"
      - "Run bootstrap procedure, verify all services deploy correctly"
      - "Migrate ArgoCD from Service annotation to Ingress pattern"
      - "Add Longhorn UI exposure via Ingress"
      - "Verify documentation accuracy (README, runbooks)"
      - "Documentation polish pass"
    exit_criteria:
      - "Recovery procedure tested end-to-end"
      - "All ArgoCD apps healthy post-recovery"
      - "README and docs verified accurate"
    completed_work:
      - "Homarr dashboard added"
      - "Netdata k8sState + CoreDNS collectors configured"
      - "CLAUDE.md updated"
      - "DR drill round 1 complete (2026-01-14): 1h17m23s, 72% context, 1 intervention"
      - "Bootstrap sequence constraints documented"
      - "Plugin updated: bootstrap-gitops.md, disaster-recovery.md, debugging.md, omni-prime.md"
      - "New reference: gitops-bootstrap-issues.md"
      - "DR drill round 2 complete (2026-01-15): 48m22s, 50% context, 0 interventions"
      - "Cilium/Talos configuration constraint added"
      - "ArgoCD Ingress added (argocd-ingress app, wave 4)"
      - "Longhorn Ingress added to apps/longhorn/"
      - "ArgoCD HA synced with Redis HA (3 replicas)"
      - "mothership-gitops README updated with full bootstrap procedure"
    known_issues:
      - issue: "ArgoCD duplicate pods after HA upgrade"
        description: "Both argocd-* (non-HA) and argocd-ha-helm-* (HA) pods running"
        cause: "HA Helm chart uses different release name, doesn't replace original"
        resolution: "Delete original argocd app from bootstrap.yaml or consolidate"
        severity: "low"
        deferred_to: "phase_6"

  phase_6:
    name: "Backup Infrastructure"
    status: "in-progress"
    started: "2026-01-16"
    depends_on: ["phase_5 (complete)"]
    tasks:
      - "Deploy MinIO on TrueNAS Scale"
      - "Create Longhorn backup bucket and IAM policy"
      - "Store MinIO credentials in Infisical"
      - "Configure Longhorn BackupTarget via ESO"
      - "Create backup schedules (critical/important/standard tiers)"
      - "Test backup and restore procedure"
    exit_criteria:
      - "MinIO accessible from cluster"
      - "Longhorn BackupTarget shows healthy"
      - "Automated backups running on schedule"
      - "Restore tested successfully"
    completed_work:
      - "MinIO deployed on TrueNAS Scale (Tailscale Serve on port 9000)"
      - "Bucket 'longhorn-backups' created with scoped IAM policy"
      - "Tailscale egress Service for MinIO (minio-truenas)"
      - "DNSConfig deployed for ts.net resolution"
      - "CoreDNS stub resolver for ts.net (volatile on Talos)"
      - "ClusterSecretStore infisical-longhorn"
      - "ExternalSecret for MinIO credentials (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_ENDPOINTS)"
      - "Longhorn BackupTarget configured and available"
      - "ArgoCD HA Redis StatefulSet fixed (was missing due to volumeClaimTemplates immutability)"
    pending_work:
      # Sequenced - each step depends on previous
      step_1:
        task: "CoreDNS SSA patch for persistent ts.net resolution"
        status: "complete"
        completed: "2026-01-17"
        note: "ArgoCD manages CoreDNS ConfigMap, survives Talos upgrades"
      step_2:
        task: "Configure Longhorn BackupTarget using native FQDN"
        status: "complete"
        completed: "2026-01-17"
        depends_on: "step_1"
        note: "AWS_ENDPOINTS uses truenas-scale.tailfb3ea.ts.net, GitOps-managed via Helm values"
      step_3:
        task: "Update Homarr to use native tailnet FQDN for Proxmox"
        status: "complete"
        completed: "2026-01-17"
        depends_on: "step_1"
        note: "Connectivity verified - egress proxies transparent when using native FQDN"
      step_4:
        task: "Create backup schedules (critical/important/standard tiers)"
        status: "complete"
        completed: "2026-01-17"
        depends_on: "step_2"
        note: "RecurringJobs deployed, homarr-database labeled for critical tier"
      step_5:
        task: "Test backup and restore procedure"
        status: "pending"
        depends_on: "step_4"
        note: "Waiting for first scheduled backup (~14:44 UTC)"
    deferred:
      - task: "Remove egress Services"
        reason: "Egress Services are transparent when using FQDN - no need to remove"
        note: "Keep for apps that need pod→tailnet connectivity"
    reference: "https://github.com/mitchross/talos-argocd-proxmox#-minio-s3-backup-configuration"

---
# REFERENCES

references:
  repo: "https://github.com/basher83/mothership-gitops"
  argocd_docs: "https://argo-cd.readthedocs.io/"
  external_secrets_infisical: "https://external-secrets.io/latest/provider/infisical/"
  tailscale_operator: "https://tailscale.com/kb/1236/kubernetes-operator"
  longhorn_docs: "https://longhorn.io/docs/"
  netdata_helm: "https://learn.netdata.cloud/docs/netdata-agent/installation/kubernetes"
  infisical_project: "mothership-s0-ew"
